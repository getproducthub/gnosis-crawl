"""
API routes for gnosis-crawl service
"""
import uuid
import logging
from typing import List
from fastapi import APIRouter, Depends, HTTPException
from datetime import datetime

from app.models import (
    CrawlRequest, CrawlResult, 
    MarkdownRequest, MarkdownResult,
    BatchRequest, BatchResult,
    JobStatus, JobListResponse
)
from app.auth import get_current_user, get_user_email
from app.crawler import get_crawler_engine

logger = logging.getLogger(__name__)

# Create API router
router = APIRouter()


@router.post("/crawl", response_model=CrawlResult)
async def crawl_single_url(
    request: CrawlRequest,
    user_email: str = Depends(get_user_email)
):
    """
    Crawl a single URL and return HTML + markdown
    Synchronous operation that returns results immediately
    """
    try:
        # Get crawler engine for this user
        crawler = await get_crawler_engine(user_email)
        
        # Perform crawl with request options
        result = await crawler.crawl_url(
            url=str(request.url),
            javascript=request.options.javascript,
            screenshot=request.options.screenshot,
            screenshot_mode=request.options.screenshot_mode,
            timeout=request.options.timeout,
            session_id=request.session_id
        )
        
        if result.success:
            return CrawlResult(
                success=True,
                url=result.url,
                html=result.html,
                markdown=result.markdown,
                metadata={
                    "title": result.title,
                    "user_email": user_email,
                    "processing_time": result.processing_time,
                    "browser_time": result.browser_time,
                    "markdown_time": result.markdown_time,
                    "page_info": result.page_info,
                    "screenshot_path": result.screenshot_path,
                    "options": request.options.dict(),
                    "session_id": request.session_id
                },
                crawled_at=datetime.utcnow()
            )
        else:
            return CrawlResult(
                success=False,
                url=result.url,
                crawled_at=datetime.utcnow(),
                error=result.error_message,
                metadata={
                    "user_email": user_email,
                    "processing_time": result.processing_time,
                    "options": request.options.dict(),
                    "session_id": request.session_id
                }
            )
        
    except Exception as e:
        logger.error(f"Failed to crawl {request.url}: {e}", exc_info=True)
        return CrawlResult(
            success=False,
            url=str(request.url),
            crawled_at=datetime.utcnow(),
            error=str(e),
            metadata={"user_email": user_email}
        )


@router.post("/markdown", response_model=MarkdownResult) 
async def crawl_markdown_only(
    request: MarkdownRequest,
    user_email: str = Depends(get_user_email)
):
    """
    Crawl a URL and return only markdown content
    Optimized for markdown extraction
    """
    try:
        # Get crawler engine for this user
        crawler = await get_crawler_engine(user_email)
        
        # Perform markdown-only crawl
        markdown_content = await crawler.crawl_for_markdown_only(
            url=str(request.url),
            javascript=request.options.javascript,
            timeout=request.options.timeout
        )
        
        # Check if crawl was successful by looking for error indicators
        if "Error" in markdown_content[:50]:  # Simple error check
            return MarkdownResult(
                success=False,
                url=str(request.url),
                crawled_at=datetime.utcnow(),
                error=markdown_content,
                metadata={"user_email": user_email}
            )
        else:
            return MarkdownResult(
                success=True,
                url=str(request.url),
                markdown=markdown_content,
                metadata={
                    "user_email": user_email,
                    "options": request.options.dict(),
                    "session_id": request.session_id
                },
                crawled_at=datetime.utcnow()
            )
        
    except Exception as e:
        logger.error(f"Failed to crawl markdown for {request.url}: {e}", exc_info=True)
        return MarkdownResult(
            success=False,
            url=str(request.url),
            crawled_at=datetime.utcnow(),
            error=str(e),
            metadata={"user_email": user_email}
        )


@router.post("/batch", response_model=BatchResult)
async def crawl_batch_urls(
    request: BatchRequest,
    user_email: str = Depends(get_user_email)
):
    """
    Start a batch crawl job for multiple URLs
    Returns results immediately (synchronous batch processing)
    """
    try:
        # Get crawler engine for this user
        crawler = await get_crawler_engine(user_email)
        
        # Generate session ID for this batch
        session_id = str(uuid.uuid4())
        
        # Convert URLs to strings
        url_list = [str(url) for url in request.urls]
        
        logger.info(f"Starting batch crawl for {len(url_list)} URLs (user: {user_email})")
        
        # Perform batch crawl (synchronous)
        batch_result = await crawler.batch_crawl(
            urls=url_list,
            javascript=request.options.javascript,
            screenshot=request.options.screenshot,
            max_concurrent=request.options.max_concurrent,
            session_id=session_id
        )
        
        return BatchResult(
            success=True,
            job_id=session_id,
            total_urls=len(url_list),
            message=f"Batch crawl completed: {batch_result['summary']['success']}/{batch_result['summary']['total']} successful",
            results=batch_result["results"],
            failed_results=batch_result["failed"],
            summary=batch_result["summary"]
        )
        
    except Exception as e:
        logger.error(f"Failed to execute batch crawl: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/jobs/{job_id}", response_model=JobStatus)
async def get_job_status(
    job_id: str,
    user_email: str = Depends(get_user_email)
):
    """
    Get status and results for a specific job
    """
    try:
        # TODO: Implement actual job status retrieval
        # For now, return a mock response for Phase 1
        
        return JobStatus(
            job_id=job_id,
            status="completed",
            progress=1.0,
            total_urls=3,
            completed_urls=3,
            results=[
                CrawlResult(
                    success=True,
                    url="https://example.com",
                    html="<html><body>Mock content</body></html>",
                    markdown="# Mock Content",
                    metadata={"title": "Example"},
                    crawled_at=datetime.utcnow()
                )
            ],
            created_at=datetime.utcnow(),
            updated_at=datetime.utcnow()
        )
        
    except Exception as e:
        logger.error(f"Failed to get job status for {job_id}: {e}")
        raise HTTPException(status_code=404, detail="Job not found")


@router.get("/jobs", response_model=JobListResponse)
async def list_user_jobs(
    user_email: str = Depends(get_user_email)
):
    """
    List all jobs for the authenticated user
    """
    try:
        # TODO: Implement actual job listing
        # For now, return a mock response for Phase 1
        
        mock_jobs = [
            {
                "job_id": "mock-job-1",
                "status": "completed",
                "total_urls": 5,
                "completed_urls": 5,
                "created_at": datetime.utcnow().isoformat()
            },
            {
                "job_id": "mock-job-2", 
                "status": "running",
                "total_urls": 10,
                "completed_urls": 7,
                "created_at": datetime.utcnow().isoformat()
            }
        ]
        
        return JobListResponse(
            jobs=mock_jobs,
            total=len(mock_jobs)
        )
        
    except Exception as e:
        logger.error(f"Failed to list jobs for user {user_email}: {e}")
        raise HTTPException(status_code=500, detail=str(e))